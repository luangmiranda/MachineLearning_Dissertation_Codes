{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9b12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.metrics import (confusion_matrix, precision_score, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score)\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_som.som import SOM\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import tree\n",
    "import glob\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f784dda",
   "metadata": {},
   "source": [
    "# Dados NSL-KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe194606",
   "metadata": {},
   "outputs": [],
   "source": [
    "kddtrain = pd.read_csv(r\"C:\\Users\\luang\\OneDrive\\Documentos\\Mestrado\\Códigos Python\\Autoencoder-based Network Anomaly Detection\\KDDTrain.csv\")\n",
    "kddtest = pd.read_csv(r\"C:\\Users\\luang\\OneDrive\\Documentos\\Mestrado\\Códigos Python\\Autoencoder-based Network Anomaly Detection\\KDDTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7152b2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp  ftp_data   SF        491          0     0   \n",
       "1         0           udp     other   SF        146          0     0   \n",
       "2         0           tcp   private   S0          0          0     0   \n",
       "3         0           tcp      http   SF        232       8153     0   \n",
       "4         0           tcp      http   SF        199        420     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
       "0               0       0    0  ...                  25   \n",
       "1               0       0    0  ...                   1   \n",
       "2               0       0    0  ...                  26   \n",
       "3               0       0    0  ...                 255   \n",
       "4               0       0    0  ...                 255   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.17                    0.03   \n",
       "1                    0.00                    0.60   \n",
       "2                    0.10                    0.05   \n",
       "3                    1.00                    0.00   \n",
       "4                    1.00                    0.00   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.17                         0.00   \n",
       "1                         0.88                         0.00   \n",
       "2                         0.00                         0.00   \n",
       "3                         0.03                         0.04   \n",
       "4                         0.00                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.00                      0.00                  0.05   \n",
       "1                  0.00                      0.00                  0.00   \n",
       "2                  1.00                      1.00                  0.00   \n",
       "3                  0.03                      0.01                  0.00   \n",
       "4                  0.00                      0.00                  0.00   \n",
       "\n",
       "   dst_host_srv_rerror_rate    class  \n",
       "0                      0.00   normal  \n",
       "1                      0.00   normal  \n",
       "2                      0.00  anomaly  \n",
       "3                      0.01   normal  \n",
       "4                      0.00   normal  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kddtrain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9746dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat((kddtrain,kddtest), axis=0)\n",
    "x = pd.get_dummies(x)\n",
    "data_class = x['class_anomaly']\n",
    "data = x.drop(['class_anomaly', 'class_normal'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c88ae554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "befca681",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_numpy()\n",
    "data_class = data_class.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7360ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9711\n",
      "12833\n"
     ]
    }
   ],
   "source": [
    "noTrain = np.count_nonzero(data_class == 0)\n",
    "print(noTrain)\n",
    "anTrain = np.count_nonzero(data_class == 1)\n",
    "print(anTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd19f9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133665\n",
      "14852\n",
      "Normal: 69348\n",
      "Anomalo: 64317\n",
      "133665\n",
      "14852\n",
      "Normal: 69348\n",
      "Anomalo: 64317\n",
      "133665\n",
      "14852\n",
      "Normal: 69348\n",
      "Anomalo: 64317\n",
      "133665\n",
      "14852\n",
      "Normal: 69348\n",
      "Anomalo: 64317\n",
      "133665\n",
      "14852\n",
      "Normal: 69349\n",
      "Anomalo: 64316\n",
      "133665\n",
      "14852\n",
      "Normal: 69349\n",
      "Anomalo: 64316\n",
      "133665\n",
      "14852\n",
      "Normal: 69349\n",
      "Anomalo: 64316\n",
      "133666\n",
      "14851\n",
      "Normal: 69349\n",
      "Anomalo: 64317\n",
      "133666\n",
      "14851\n",
      "Normal: 69349\n",
      "Anomalo: 64317\n",
      "133666\n",
      "14851\n",
      "Normal: 69349\n",
      "Anomalo: 64317\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for train, test in kfold.split(data, data_class):\n",
    "    print(train.shape[0])\n",
    "    print(test.shape[0])\n",
    "    noTrain = np.count_nonzero(data_class[train] == 0)\n",
    "    print(\"Normal:\",noTrain)\n",
    "    anTrain = np.count_nonzero(data_class[train] == 1)\n",
    "    print(\"Anomalo:\",anTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d15fc3",
   "metadata": {},
   "source": [
    "# Abordagem Autoencoder-Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34590a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000182810AC438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000182810AC438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 3ms/step - loss: 0.0551\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0447\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 0.0311A: 0s - loss: 0.03\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 0.0261\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 0.0222\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0105\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0054\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0044A: 0s - loss: 0.004\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036A: 0s - loss: 0.00 - ETA: 0s - loss: 0.0\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0035A: 0s - loss: \n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 0.0025\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023A: 0s - loss: 0\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018283374F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018283374F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018283374948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018283374948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 3ms/step - loss: 0.0407\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0240\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0203\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0190A: 0s - loss: 0.01\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0182\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0134\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0099\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0094\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0091\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0088\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0086\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0084\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074A: 0s - loss: \n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018283A69B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018283A69B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281B25C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281B25C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 2ms/step - loss: 0.0286\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0091\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0089\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0088\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0087\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0085\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0085\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0081\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080A: 0s - loss: \n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 0.0079\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018281DF6708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018281DF6708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281E59948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281E59948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 2ms/step - loss: 0.0357\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0233A: 0s - loss: 0.023\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0216\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0205\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0192\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0182\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0181\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0181\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0181\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0180\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0180\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0179\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0144\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0116\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0115\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0114A: 0s - loss:\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0113\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0113\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111A: 0s - loss: \n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111A: 0s - loss: 0.0\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0110\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0110\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0110\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0110\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0110\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0110\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001828206C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001828206C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281FC2048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281FC2048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 2ms/step - loss: 0.0275\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0036\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.003 - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000182824A7798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000182824A7798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018282E52558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018282E52558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 2ms/step - loss: 0.0331\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0101\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0096\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0092\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0084\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0076A: 0s - loss: 0.0\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073A: 0s - loss: 0.007\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 0.0072\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071A: 0s - loss: 0.00\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018282397948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018282397948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281AB0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281AB0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 2ms/step - loss: 0.0282\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018283A09678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018283A09678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018284045558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018284045558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 2ms/step - loss: 0.0384\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0218\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0162\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0154A: 0s - loss: 0\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0154\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0096\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.009 - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 0.0093\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018281AAC168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018281AAC168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281AFA828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018281AFA828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1s 2ms/step - loss: 0.0399\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0262\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.008 - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018281E24EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018281E24EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000182821D3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000182821D3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 2ms/step - loss: 0.0317\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0173\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0042\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018281C244C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018281C244C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 256\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "activation = \"relu\"\n",
    "neighbors = 11\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "count = 0\n",
    "\n",
    "#Métricas ErroRecons\n",
    "matrix_ErroRecons = np.zeros((num_folds,2,2))\n",
    "accuracy_ErroRecons = np.zeros(num_folds)\n",
    "recall_ErroRecons = np.zeros(num_folds)\n",
    "precision_ErroRecons = np.zeros(num_folds)\n",
    "f1_ErroRecons = np.zeros(num_folds)\n",
    "Z = np.arange(0.000,4.095,0.005)\n",
    "bestZ = np.zeros(num_folds)\n",
    "bestTeta = np.zeros(num_folds)\n",
    "bestmean = np.zeros(num_folds)\n",
    "bestdesv = np.zeros(num_folds)\n",
    "bestErrorNumber = np.zeros(num_folds)\n",
    "ErrorNumberFolds = np.zeros((num_folds,1,Z.shape[0]-1))\n",
    "\n",
    "#Métricas KNN\n",
    "matrix_knn = np.zeros((num_folds,2,2))\n",
    "accuracy_knn = np.zeros(num_folds)\n",
    "recall_knn = np.zeros(num_folds)\n",
    "precision_knn = np.zeros(num_folds)\n",
    "f1_knn = np.zeros(num_folds)\n",
    "\n",
    "#Métricas Decision Tree\n",
    "matrix_dt = np.zeros((num_folds,2,2))\n",
    "accuracy_dt = np.zeros(num_folds)\n",
    "recall_dt = np.zeros(num_folds)\n",
    "precision_dt = np.zeros(num_folds)\n",
    "f1_dt = np.zeros(num_folds)\n",
    "\n",
    "#Métricas SVM\n",
    "matrix_svm = np.zeros((num_folds,2,2))\n",
    "accuracy_svm = np.zeros(num_folds)\n",
    "recall_svm = np.zeros(num_folds)\n",
    "precision_svm = np.zeros(num_folds)\n",
    "f1_svm = np.zeros(num_folds)\n",
    "\n",
    "for train, test in kfold.split(data, data_class):\n",
    "    \n",
    "    #####################################################\n",
    "    ####Pré-Processamento\n",
    "    \n",
    "    #Separação dos Folds de Treinamento e Teste\n",
    "    dataTrain = data[train]\n",
    "    dataTrain_class = data_class[train]\n",
    "    modelTest = data[test]\n",
    "    modelTest_class = data_class[test]\n",
    "    \n",
    "    #Conta Quantidade de dados\n",
    "    Train = dataTrain.shape[1]\n",
    "    noTrain = np.count_nonzero(dataTrain_class == 0)\n",
    "    anTrain = np.count_nonzero(dataTrain_class == 1)\n",
    "\n",
    "    #Separa dados normais e anômalos\n",
    "    normalTrain = dataTrain[np.where(dataTrain_class == 0)]\n",
    "    anomalyTrain = dataTrain[np.where(dataTrain_class == 1)]\n",
    "    \n",
    "    #Divide dados para treinamento\n",
    "    porcen = 0.50\n",
    "    j = int(noTrain * porcen)\n",
    "    \n",
    "    #Índices escolhidos aleatoriamente\n",
    "    numbers_knn_normal = np.array(random.sample(range(0, noTrain),j))\n",
    "    numbers_knn_anomaly = np.array(random.sample(range(0,anTrain),j))\n",
    "    numbers_autoencoder_normal = np.array(list(set(np.arange(0, noTrain)) - set(numbers_knn_normal)))\n",
    "    \n",
    "    #Dados KNN\n",
    "    knnTrain_normal = normalTrain[numbers_knn_normal]\n",
    "    knnTrain_anomaly = anomalyTrain[numbers_knn_anomaly]\n",
    "    knnTrain = np.concatenate((knnTrain_normal, knnTrain_anomaly), axis=0)\n",
    "    knnTrain_class = np.concatenate((np.zeros(j), np.ones(j)))\n",
    "    \n",
    "    #Dados Autoencoder\n",
    "    autoencoderTrain = normalTrain[numbers_autoencoder_normal]\n",
    "    autoencoderTrain_class = np.zeros(autoencoderTrain.shape[0])\n",
    "    \n",
    "    #Normalização\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(knnTrain)\n",
    "    autoencoderTrain = scaler.transform(autoencoderTrain)\n",
    "    knnTrain = scaler.transform(knnTrain)\n",
    "    modelTest = scaler.transform(modelTest)\n",
    "    \n",
    "    #####################################################\n",
    "    ####Autoencoder\n",
    "    #Modela as camadas do Autoencoder\n",
    "    var = autoencoderTrain.shape[1]\n",
    "    input_vector = keras.Input(shape=(var,))\n",
    "    en1 = layers.Dense(32, activation=activation)(input_vector)\n",
    "    en2 = layers.Dense(16, activation=activation)(en1)\n",
    "    de1 = layers.Dense(32, activation=activation)(en2)\n",
    "    de2 = layers.Dense(var, activation=activation)(de1)\n",
    "\n",
    "    #Gera o modelo\n",
    "    autoencoder = keras.Model(input_vector, de2)\n",
    "\n",
    "    #Compila o modelo\n",
    "    autoencoder.compile(optimizer=optimizer, loss=\"mean_squared_error\")\n",
    "\n",
    "    #Treina o modelo\n",
    "    history = autoencoder.fit(autoencoderTrain, autoencoderTrain, epochs = epochs, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    #####################################################\n",
    "    ####Erro de Reconstrução\n",
    "    #Teste\n",
    "    pred_ErroRecons = autoencoder.predict(modelTest)\n",
    "    \n",
    "    #Extração do erro\n",
    "    mse_ErroRecons = np.mean(np.power(modelTest - pred_ErroRecons, 2), axis=1)\n",
    "    error_df_ErroRecons = pd.DataFrame({'reconstruction_error': mse_ErroRecons,\n",
    "                                        'true_class': modelTest_class})\n",
    "\n",
    "    media = np.mean(error_df_ErroRecons['reconstruction_error'])\n",
    "    desv = np.std(error_df_ErroRecons['reconstruction_error'])\n",
    "\n",
    "    #Determinação do limiar ótimo\n",
    "\n",
    "    ErrosLimiar = np.zeros(Z.shape[0]-1)\n",
    "    melhorTeta = 0\n",
    "    melhorZ = 0\n",
    "    prediMatrix = np.zeros((Z.shape[0]-1, 1, modelTest.shape[0]))\n",
    "    \n",
    "    for i in range(0, Z.shape[0]-1):\n",
    "        #Obtenção do Limiar\n",
    "        teta = media + Z[i]*desv\n",
    "    \n",
    "        #Predição\n",
    "        predi_ErroRecons = np.ones(modelTest.shape[0])\n",
    "        error_ErroRecons = error_df_ErroRecons.to_numpy()\n",
    "        predi_ErroRecons[np.where(error_ErroRecons[:,0] <= teta)] = 0\n",
    "        prediMatrix[i] = predi_ErroRecons \n",
    "        \n",
    "        #Contagem da quantidade de erros obtidos para cada Z\n",
    "        numberErros = 0\n",
    "        for j in range(0, predi_ErroRecons.shape[0]):\n",
    "            if error_ErroRecons[j,1] != predi_ErroRecons[j]:\n",
    "                numberErros = numberErros + 1\n",
    "        ErrosLimiar[i] = numberErros\n",
    "        \n",
    "    predi_ErroRecons = prediMatrix[np.argpartition(ErrosLimiar,1)[0]]\n",
    "            \n",
    "    #Métricas\n",
    "    accuracy_ErroRecons[count] = accuracy_score(error_ErroRecons[:,1], predi_ErroRecons[:].reshape(-1,1))\n",
    "    recall_ErroRecons[count] = recall_score(error_ErroRecons[:,1], predi_ErroRecons[:].reshape(-1,1))\n",
    "    precision_ErroRecons[count] = precision_score(error_ErroRecons[:,1], predi_ErroRecons[:].reshape(-1,1))\n",
    "    f1_ErroRecons[count] = f1_score(error_ErroRecons[:,1], predi_ErroRecons[:].reshape(-1,1))\n",
    "    matrix_ErroRecons[count] = confusion_matrix(error_ErroRecons[:,1], predi_ErroRecons[:].reshape(-1,1))\n",
    "    bestZ[count] = Z[np.argpartition(ErrosLimiar,1)[0]]\n",
    "    bestTeta[count] = media + Z[np.argpartition(ErrosLimiar,1)[0]]*desv\n",
    "    bestmean[count] = media\n",
    "    bestdesv[count] = desv\n",
    "    bestErrorNumber[count] = ErrosLimiar[np.argpartition(ErrosLimiar,1)[0]]\n",
    "    ErrorNumberFolds[count] = ErrosLimiar\n",
    "    \n",
    "    #####################################################\n",
    "    ####KNN\n",
    "    ###Treinamento KNN\n",
    "    pred_knn = autoencoder.predict(knnTrain)\n",
    "    mse_knn = np.mean(np.power(knnTrain - pred_knn, 2), axis=1)\n",
    "    error_df_knn = pd.DataFrame({'reconstruction_error': mse_knn,\n",
    "                                 'true_class': knnTrain_class})\n",
    "    error_knn = error_df_knn.to_numpy()\n",
    "    neigh = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    neigh.fit(error_knn[:,0].reshape(-1,1), error_knn[:,1])\n",
    "    \n",
    "    #Teste do Modelo\n",
    "    pred_knn = autoencoder.predict(modelTest)\n",
    "    mse_knn = np.mean(np.power(modelTest - pred_knn, 2), axis=1)\n",
    "    error_df_knn = pd.DataFrame({'reconstruction_error': mse_knn,\n",
    "                                'true_class': modelTest_class})\n",
    "    error_knn = error_df_knn.to_numpy()\n",
    "    predict_knn = neigh.predict(error_knn[:,0].reshape(-1,1))\n",
    "    \n",
    "    #Métricas\n",
    "    accuracy_knn[count] = accuracy_score(error_knn[:,1], predict_knn[:])\n",
    "    recall_knn[count] = recall_score(error_knn[:,1], predict_knn[:])\n",
    "    precision_knn[count] = precision_score(error_knn[:,1], predict_knn[:])\n",
    "    f1_knn[count] = f1_score(error_knn[:,1], predict_knn[:])\n",
    "    matrix_knn[count] = confusion_matrix(error_knn[:,1], predict_knn[:])\n",
    "    \n",
    "   \n",
    "    \n",
    "    #####################################################\n",
    "    ####Decision Tree\n",
    "    ###Treinamento Decision Tree \n",
    "    pred_dt = autoencoder.predict(knnTrain)\n",
    "    mse_dt = np.mean(np.power(knnTrain - pred_dt, 2), axis=1)\n",
    "    error_df_dt = pd.DataFrame({'reconstruction_error': mse_dt,\n",
    "                                     'true_class': knnTrain_class})\n",
    "    error_dt = error_df_dt.to_numpy()\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt.fit(error_dt[:,0].reshape(-1,1), error_dt[:,1])\n",
    "\n",
    "    #Teste\n",
    "    pred_dt = autoencoder.predict(modelTest)\n",
    "    mse_dt = np.mean(np.power(modelTest - pred_dt, 2), axis=1)\n",
    "    error_df_dt = pd.DataFrame({'reconstruction_error': mse_dt,\n",
    "                                     'true_class': modelTest_class})\n",
    "    error_dt = error_df_dt.to_numpy()\n",
    "    predict_dt = dt.predict(error_dt[:,0].reshape(-1,1))\n",
    "\n",
    "    #Métricas\n",
    "    accuracy_dt[count] = accuracy_score(error_dt[:,1], predict_dt[:])\n",
    "    recall_dt[count] = recall_score(error_dt[:,1], predict_dt[:])\n",
    "    precision_dt[count] = precision_score(error_dt[:,1], predict_dt[:])\n",
    "    f1_dt[count] = f1_score(error_dt[:,1], predict_dt[:])\n",
    "    matrix_dt[count] = confusion_matrix(error_dt[:,1], predict_dt[:])\n",
    "    \n",
    "   \n",
    "\n",
    "    #####################################################\n",
    "    ####SVM\n",
    "    #Treinamento SVM\n",
    "    pred_svm = autoencoder.predict(knnTrain)\n",
    "    mse_svm = np.mean(np.power(knnTrain - pred_svm, 2), axis=1)\n",
    "    error_df_svm = pd.DataFrame({'reconstruction_error': mse_svm,\n",
    "                                 'true_class': knnTrain_class})\n",
    "    error_svm = error_df_svm.to_numpy()\n",
    "    clf = svm.SVC(kernel='rbf')\n",
    "    clf.fit(error_svm[:,0].reshape(-1,1), error_svm[:,1])\n",
    "\n",
    "    #Teste\n",
    "    pred_svm = autoencoder.predict(modelTest)\n",
    "    mse_svm = np.mean(np.power(modelTest - pred_svm, 2), axis=1)\n",
    "    error_df_svm = pd.DataFrame({'reconstruction_error': mse_svm,\n",
    "                                 'true_class': modelTest_class})\n",
    "    error_svm = error_df_svm.to_numpy()\n",
    "    predict_svm = clf.predict(error_svm[:,0].reshape(-1,1))\n",
    "    \n",
    "    #Métricas\n",
    "    accuracy_svm[count] = accuracy_score(error_svm[:,1], predict_svm[:])\n",
    "    recall_svm[count] = recall_score(error_svm[:,1], predict_svm[:])\n",
    "    precision_svm[count] = precision_score(error_svm[:,1], predict_svm[:])\n",
    "    f1_svm[count] = f1_score(error_svm[:,1], predict_svm[:])\n",
    "    matrix_svm[count] = confusion_matrix(error_svm[:,1], predict_svm[:])\n",
    "    \n",
    "    \n",
    "    \n",
    "      \n",
    "    count = count + 1\n",
    "    fold_no = fold_no + 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c426a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder - Limiar Referência:\n",
      " [[7369.7  335.7]\n",
      " [1326.5 5819.8]] \n",
      "Acurácia:  0.88808017572815  +/-  0.023474395009095303 \n",
      "Recall:  0.8143791149132383  +/-  0.04266982311105274 \n",
      "Precision:  0.9464743955677047  +/-  0.03060818822748716 \n",
      "F1:  0.8746419530529158  +/-  0.027371113175074333\n",
      "\n",
      "Fold  1 :  0.01660062316075063 = 0.01660062316075063 + 0.0 * 0.018853169960511673 \n",
      "\n",
      "Número de Erros:  1429.0\n",
      "\n",
      "Fold  2 :  0.017636196765374302 = 0.017636196765374302 + 0.0 * 0.01406899230389916 \n",
      "\n",
      "Número de Erros:  1617.0\n",
      "\n",
      "Fold  3 :  0.014564876646302833 = 0.014564876646302833 + 0.0 * 0.010111435008653588 \n",
      "\n",
      "Número de Erros:  2234.0\n",
      "\n",
      "Fold  4 :  0.018168802463172883 = 0.017836986516696428 + 0.035 * 0.009480455613612965 \n",
      "\n",
      "Número de Erros:  1884.0\n",
      "\n",
      "Fold  5 :  0.012044189263176327 = 0.011875866787267918 + 0.015 * 0.01122149839389386 \n",
      "\n",
      "Número de Erros:  1116.0\n",
      "\n",
      "Fold  6 :  0.01570527731128143 = 0.01570527731128143 + 0.0 * 0.012401236272478756 \n",
      "\n",
      "Número de Erros:  1664.0\n",
      "\n",
      "Fold  7 :  0.017536031734078646 = 0.017536031734078646 + 0.0 * 0.015247360695321049 \n",
      "\n",
      "Número de Erros:  1743.0\n",
      "\n",
      "Fold  8 :  0.017909920201358235 = 0.017909920201358235 + 0.0 * 0.01199630646483117 \n",
      "\n",
      "Número de Erros:  1885.0\n",
      "\n",
      "Fold  9 :  0.019571591868861087 = 0.019571591868861087 + 0.0 * 0.01803203778593204 \n",
      "\n",
      "Número de Erros:  1967.0\n",
      "\n",
      "Fold  10 :  0.013784667500593628 = 0.013784667500593628 + 0.0 * 0.013225648055396659 \n",
      "\n",
      "Número de Erros:  1083.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Autoencoder - Limiar Referência:\\n\",np.mean(matrix_ErroRecons, axis=0), \"\\nAcurácia: \", np.mean(accuracy_ErroRecons),' +/- ',np.std(accuracy_ErroRecons), \"\\nRecall: \", np.mean(recall_ErroRecons),' +/- ',np.std(recall_ErroRecons), \"\\nPrecision: \", np.mean(precision_ErroRecons),' +/- ',np.std(precision_ErroRecons), \"\\nF1: \", np.mean(f1_ErroRecons),' +/- ',np.std(f1_ErroRecons))\n",
    "for i in range(0,fold_no-1):\n",
    "    print(\"\\nFold \",i+1,\": \" , bestTeta[i], \"=\", bestmean[i], \"+\", bestZ[i], \"*\", bestdesv[i], \"\\n\\nNúmero de Erros: \", bestErrorNumber[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a03d2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder - KNN:\n",
      " [[7183.4  522. ]\n",
      " [ 642.8 6503.5]] \n",
      "Acurácia:  0.9215713496501735  +/-  0.01227403996613153 \n",
      "Recall:  0.9100514989858919  +/-  0.015888533156893418 \n",
      "Precision:  0.9257289596253313  +/-  0.012668093871829086 \n",
      "F1:  0.9177814427313754  +/-  0.013017695898368417\n",
      "\n",
      "Autoencoder - SVM:\n",
      " [[7278.8  426.6]\n",
      " [1131.7 6014.6]] \n",
      "Acurácia:  0.8950758248335295  +/-  0.019576319535779953 \n",
      "Recall:  0.8416380396934848  +/-  0.03865548219485491 \n",
      "Precision:  0.9347300740269249  +/-  0.026088568522749166 \n",
      "F1:  0.8850087967873236  +/-  0.022876965520127353\n",
      "\n",
      "Autoencoder - Decision Tree:\n",
      " [[7188.6  516.8]\n",
      " [ 814.2 6332.1]] \n",
      "Acurácia:  0.9103807217770676  +/-  0.015145223933448955 \n",
      "Recall:  0.8860668866913054  +/-  0.017470157009622032 \n",
      "Precision:  0.9245333979870856  +/-  0.015705904148126226 \n",
      "F1:  0.9048735769388244  +/-  0.016185213721226176\n"
     ]
    }
   ],
   "source": [
    "print(\"Autoencoder - KNN:\\n\", np.mean(matrix_knn, axis=0), \"\\nAcurácia: \", np.mean(accuracy_knn), ' +/- ', np.std(accuracy_knn), \"\\nRecall: \", np.mean(recall_knn), ' +/- ', np.std(recall_knn), \"\\nPrecision: \", np.mean(precision_knn), ' +/- ', np.std(precision_knn), \"\\nF1: \", np.mean(f1_knn), ' +/- ', np.std(f1_knn))\n",
    "\n",
    "print(\"\\nAutoencoder - SVM:\\n\", np.mean(matrix_svm, axis=0), \"\\nAcurácia: \", np.mean(accuracy_svm), ' +/- ', np.std(accuracy_svm), \"\\nRecall: \", np.mean(recall_svm), ' +/- ', np.std(recall_svm), \"\\nPrecision: \", np.mean(precision_svm), ' +/- ', np.std(precision_svm), \"\\nF1: \", np.mean(f1_svm), ' +/- ', np.std(f1_svm))\n",
    "\n",
    "print(\"\\nAutoencoder - Decision Tree:\\n\", np.mean(matrix_dt, axis=0), \"\\nAcurácia: \", np.mean(accuracy_dt), ' +/- ', np.std(accuracy_dt), \"\\nRecall: \", np.mean(recall_dt), ' +/- ', np.std(recall_dt), \"\\nPrecision: \", np.mean(precision_dt), ' +/- ', np.std(precision_dt), \"\\nF1: \", np.mean(f1_dt), ' +/- ', np.std(f1_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ffb8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luang\\anaconda3\\envs\\keras_env\\lib\\site-packages\\ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "Result_GridSearchLimiar = np.array([\"Theta: \" , bestTeta,\"\",\n",
    "                                \"Média: \", bestmean,\"\",\n",
    "                                \"Valor Z: \", bestZ,\"\",\n",
    "                                \"Desvio Padrao: \", bestdesv,\"\",\n",
    "                                \"Número de Erros: \", bestErrorNumber,\"\",\n",
    "                                \"Erros por Fold: \", ErrorNumberFolds])\n",
    "\n",
    "np.savetxt('Result_GridSearchLimiarRef_Error.txt', Result_GridSearchLimiar, delimiter = ',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e043b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luang\\anaconda3\\envs\\keras_env\\lib\\site-packages\\ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "Result = np.array([\"Referência\", \"Acurácia\", accuracy_ErroRecons, np.mean(accuracy_ErroRecons), np.std(accuracy_ErroRecons),\"\",\n",
    "                                \"Recall\", recall_ErroRecons, np.mean(recall_ErroRecons), np.std(recall_ErroRecons), \"\",\n",
    "                                \"Precision\", precision_ErroRecons, np.mean(precision_ErroRecons), np.std(precision_ErroRecons), \"\",\n",
    "                                \"F1\", f1_ErroRecons, np.mean(f1_ErroRecons), np.std(f1_ErroRecons), \"-----------------\",\"-----------------\",\n",
    "                   \"KNN\", \"Acurácia\", accuracy_knn, np.mean(accuracy_knn), np.std(accuracy_knn),\"\",\n",
    "                                \"Recall\", recall_knn, np.mean(recall_knn), np.std(recall_knn), \"\",\n",
    "                                \"Precision\", precision_knn, np.mean(precision_knn), np.std(precision_knn), \"\",\n",
    "                                \"F1\", f1_knn, np.mean(f1_knn), np.std(f1_knn), \"-----------------\",\"-----------------\",\n",
    "                  \"Decision Tree\",  \"Acurácia\", accuracy_dt, np.mean(accuracy_dt), np.std(accuracy_dt),\"\",\n",
    "                                \"Recall\", recall_dt, np.mean(recall_dt), np.std(recall_dt), \"\",\n",
    "                                \"Precision\", precision_dt, np.mean(precision_dt), np.std(precision_dt), \"\",\n",
    "                                \"F1\", f1_dt, np.mean(f1_dt), np.std(f1_dt), \"-----------------\",\"-----------------\",\n",
    "                  \"SVM\",  \"Acurácia\", accuracy_svm, np.mean(accuracy_svm), np.std(accuracy_svm),\"\",\n",
    "                                \"Recall\", recall_svm, np.mean(recall_svm), np.std(recall_svm), \"\",\n",
    "                                \"Precision\", precision_svm, np.mean(precision_svm), np.std(precision_svm), \"\",\n",
    "                                \"F1\", f1_svm, np.mean(f1_svm), np.std(f1_svm), \"\",\"\",])\n",
    "\n",
    "np.savetxt('Result_Error.txt', Result, delimiter = ',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b84b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luang\\anaconda3\\envs\\keras_env\\lib\\site-packages\\ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "Result_Matrix = np.array([\"Referência\", matrix_ErroRecons, np.mean(matrix_ErroRecons, axis=0), np.std(matrix_ErroRecons, axis=0), \"-----------------\",\"-----------------\",\n",
    "                          \"KNN\", matrix_knn, np.mean(matrix_knn, axis=0), np.std(matrix_knn, axis=0), \"-----------------\",\"-----------------\",\n",
    "                          \"Decision Tree\", matrix_dt, np.mean(matrix_dt, axis=0), np.std(matrix_dt, axis=0), \"-----------------\",\"-----------------\",\n",
    "                          \"SVM\", matrix_svm, np.mean(matrix_svm, axis=0), np.std(matrix_svm, axis=0)])\n",
    "\n",
    "np.savetxt('Result_Error_Matrix.txt', Result_Matrix, delimiter = ',',fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
